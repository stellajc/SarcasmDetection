{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SarcasmDetection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhmXXAPv85v0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer, TFBertModel, BertModel\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve,f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import os\n",
        "import bert\n",
        "# from keras_bert import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional,GlobalMaxPooling1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import XLNetTokenizer, XLNetModel,TFXLNetModel,get_linear_schedule_with_warmup\n",
        "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pytorch_transformers import XLNetConfig\n",
        "# from pytorch_transformers import XLNetTokenizer\n",
        "from pytorch_transformers import XLNetForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import tf_slim as slim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler, Dataset\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "# drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install tf-models-official\n",
        "!pip3 install tf_slim\n",
        "!pip3 install transformers\n",
        "!pip3 install bert\n",
        "!pip3 install pytorch_transformers"
      ],
      "metadata": {
        "id": "12zWl2xabUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARC dataset"
      ],
      "metadata": {
        "id": "wl4-LOWKgFHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train-balanced.csv\",sep='\\t')\n",
        "train.columns=['label','comment','author','subreddit','score','ups','downs','date','created_utc','parent_comment']\n",
        "# t.columns = ['parent','response','label0','label1']\n",
        "# t['parent'].str.split('|', expand=True)\n",
        "train[:10]"
      ],
      "metadata": {
        "id": "448VrBlk-GMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test-balanced.csv\",sep='\\t')\n",
        "test.columns=['label','comment','author','subreddit','score','ups','downs','date','created_utc','parent_comment']\n",
        "test[:10]"
      ],
      "metadata": {
        "id": "YwBy6TGT--d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean data\n",
        "train = train.dropna()\n",
        "train = train.reset_index()\n",
        "test = test.dropna()\n",
        "test = test.reset_index()\n",
        "comments = [train['parent_comment'][i] + '[SEP]' + train['comment'][i] for i in range(len(train))]\n",
        "train['inputs'] = comments"
      ],
      "metadata": {
        "id": "8GTdoDbuASGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in comments]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 60)"
      ],
      "metadata": {
        "id": "Ly2XLOSiEuYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtained from graph of last cell\n",
        "max_seq_len = 64"
      ],
      "metadata": {
        "id": "eWwSVru_E0qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(train['inputs'], train['label'], \n",
        "                                                                    random_state=2022, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=train['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2022, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "yImLrp_uEZiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lee etc's Model\n",
        "Ref: \n",
        "*   BERT: https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb\n",
        "*   NeXtVLAD: https://github.com/zhangyaoyuan/NextVLAD-Attention-Model/blob/master/NeXtVLADModelLF.py\n",
        "\n"
      ],
      "metadata": {
        "id": "awtAnWl0gTg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers.utils.dummy_pt_objects import BertModel\n",
        "# bert = BertModel.from_pretrained('bert-large-cased')\n",
        "bert = AutoModel.from_pretrained('bert-large-cased')\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased')"
      ],
      "metadata": {
        "id": "eT8GI-prFVkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "id": "9OR0lZVEE37r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "RuIk449OE6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a batch size\n",
        "batch_size = 4\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "5jiwkn_NE_bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "sRIAZxM9Fp5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## OWN\n",
        "lr = 1e-6\n",
        "batch_size = 4\n",
        "dropout = 0.25\n",
        "max_document_length = 50  # each sentence has until 100 words\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "# max_size = 5000 # maximum vocabulary size\n",
        "seed = 27\n",
        "num_classes = 2\n",
        "lstm_units = 1024\n",
        "lstm_layers = 2  # LSTM layers\n",
        "num_epochs = 4\n",
        "lstm_out = 128\n",
        "cluster_size = 512  # K\n",
        "expansion = 4    # lambda\n",
        "groups = 8       # G\n",
        "output_size = 2\n",
        "feature_size = 128  # N\n",
        "max_frames = 4 "
      ],
      "metadata": {
        "id": "odlvcTSfOxmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Partial codes from original model but reconstructured based on Lee et al.\n",
        "class Bert_BiLSTM_NeXtVLAD(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(Bert_BiLSTM_NeXtVLAD, self).__init__()\n",
        "\n",
        "      self.bert = bert\n",
        "\n",
        "      # LSTM layers\n",
        "      self.bilstm = nn.LSTM(128,\n",
        "                            lstm_units,\n",
        "                            num_layers=lstm_layers,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "      \n",
        "      # nextvlad params\n",
        "      self.cluster_size = cluster_size\n",
        "      self.expansion = expansion\n",
        "      self.groups = groups\n",
        "      self.max_frames = max_frames  # M\n",
        "      self.feature_size = feature_size\n",
        "      self.is_training = True\n",
        "\n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = (Variable(torch.zeros(lstm_layers * 2, batch_size, lstm_units)),\n",
        "                Variable(torch.zeros(lstm_layers * 2, batch_size, lstm_units)))\n",
        "        return hidden\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "      batch_size = sent_id.size(0)\n",
        "\n",
        "      # pass the inputs to the bert  \n",
        "      # x = self.bert(sent_id)\n",
        "      _, x = self.bert(sent_id, attention_mask=mask, return_dict = False)\n",
        "\n",
        "      # construct bilstm\n",
        "      hidden = self.init_hidden(batch_size)\n",
        "\n",
        "      lstm_out, (hidden_last,cn_last) = self.bilstm(x, hidden)\n",
        "\n",
        "      hidden_last_L = hidden_last[-2]\n",
        "      \n",
        "      hidden_last_R = hidden_last[-1]\n",
        "      \n",
        "      hidden_last_out = torch.cat([hidden_last_L,hidden_last_R],dim=-1)\n",
        "\n",
        "      x = self.dropout(hidden_last_out)[:max_frames]\n",
        "\n",
        "      # construct nextvlad\n",
        "      x = slim.fully_connected(x, self.expansion * self.feature_size, activation_fn=None,\n",
        "                                    weights_initializer=slim.variance_scaling_initializer())\n",
        "\n",
        "      attention = slim.fully_connected(x, self.groups, activation_fn=tf.nn.sigmoid,\n",
        "                                         weights_initializer=slim.variance_scaling_initializer())\n",
        "      \n",
        "      attention = tf.reshape(attention, [-1, self.max_frames*self.groups, 1])\n",
        "\n",
        "      tf.summary.histogram(\"sigmoid_attention\", attention)\n",
        "      \n",
        "      feature_size = self.expansion * self.feature_size // self.groups\n",
        "\n",
        "      cluster_weights = tf.get_variable(\"cluster_weights\",\n",
        "                                        [self.expansion*self.feature_size, self.groups*self.cluster_size],\n",
        "                                        initializer=slim.variance_scaling_initializer()\n",
        "                                        )\n",
        "\n",
        "      # tf.summary.histogram(\"cluster_weights\", cluster_weights)\n",
        "      reshaped_input = tf.reshape(input, [-1, self.expansion * self.feature_size])\n",
        "      activation = tf.matmul(reshaped_input, cluster_weights)\n",
        "\n",
        "      activation = slim.batch_norm(\n",
        "          activation,\n",
        "          center=True,\n",
        "          scale=True,\n",
        "          is_training=self.is_training,\n",
        "          scope=\"cluster_bn\",\n",
        "          fused=False)\n",
        "\n",
        "      activation = tf.reshape(activation, [-1, self.max_frames * self.groups, self.cluster_size])\n",
        "      activation = tf.nn.softmax(activation, axis=-1)\n",
        "      activation = tf.multiply(activation, attention)\n",
        "      # tf.summary.histogram(\"cluster_output\", activation)\n",
        "      a_sum = tf.reduce_sum(activation, -2, keep_dims=True)\n",
        "\n",
        "      cluster_weights2 = tf.get_variable(\"cluster_weights2\",\n",
        "                                          [1, feature_size, self.cluster_size],\n",
        "                                          initializer=slim.variance_scaling_initializer()\n",
        "                                          )\n",
        "      a = tf.multiply(a_sum, cluster_weights2)\n",
        "\n",
        "      activation = tf.transpose(activation, perm=[0, 2, 1])\n",
        "\n",
        "      reshaped_input = tf.reshape(input, [-1, self.max_frames * self.groups, feature_size])\n",
        "      vlad = tf.matmul(activation, reshaped_input)\n",
        "      vlad = tf.transpose(vlad, perm=[0, 2, 1])\n",
        "      vlad = tf.subtract(vlad, a)\n",
        "\n",
        "      vlad = tf.nn.l2_normalize(vlad, 1)\n",
        "\n",
        "      vlad = tf.reshape(vlad, [-1, self.cluster_size * feature_size])\n",
        "      vlad = slim.batch_norm(vlad,\n",
        "              center=True,\n",
        "              scale=True,\n",
        "              is_training=self.is_training,\n",
        "              scope=\"vlad_bn\",\n",
        "              fused=False)\n",
        "\n",
        "\n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "6P2cr51pFqU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = Bert_BiLSTM_NeXtVLAD(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "I5E9aIILFsWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "pIXV86QWFv4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight ='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "metadata": {
        "id": "W4bbbCmeGOSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.CrossEntropyLoss(weight= weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 4"
      ],
      "metadata": {
        "id": "0mOppkIeGLO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "8MD9wv1-F3Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      # elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "W2E97_jMF516"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "Oc6ntCb1F9He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load weights of best model\n",
        "# path = 'saved_weights.pt'\n",
        "# model.load_state_dict(torch.load(path))\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# F1 evaluate\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "id": "0xxGjNEJQsPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataData Scrapping "
      ],
      "metadata": {
        "id": "gOCyWCY4Gm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - Neutral, 1 - Postive, -1 - Negative\n",
        "sent_data = pd.read_csv(\"Reddit_Data.csv\")\n",
        "sent_data=sent_data.dropna()\n",
        "sent_data[:10]"
      ],
      "metadata": {
        "id": "JARz9taXr6uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences  = []\n",
        "sent_data = sent_data[sent_data['category'] !=0]\n",
        "# for sentence in sent_data['clean_comment']:\n",
        "#   sentence = sentence + \"[SEP] [CLS]\"\n",
        "#   sentences.append(sentence)\n",
        "labels = sent_data['category'].values\n",
        "sentences = list(sent_data['clean_comment'])"
      ],
      "metadata": {
        "id": "OWBWQEjdsX3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "metadata": {
        "id": "OgSEq7Q9I2Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128 \n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2022, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2022, test_size=0.1)\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "Hlzq8Tc6JZWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "PruF0R1nJwNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "4DP_7nBUJ1_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Model - XLNet\n",
        "\n",
        "* XLNet: https://www.kaggle.com/code/rihab147/xlnet-tunisian-sentiment-analysis/notebook & https://github.com/zihangdai/xlnet\n",
        "\n"
      ],
      "metadata": {
        "id": "MtrlTw_8rgPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only needs to be done once\n",
        "# ! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "# ! unzip cased_L-24_H-1024_A-16.zip \n",
        "# ! git clone https://github.com/zihangdai/xlnet.git"
      ],
      "metadata": {
        "id": "gX4HJPiGuQvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCRIPTS_DIR = 'xlnet' #@param {type:\"string\"}\n",
        "DATA_DIR = 'DATAPATH' #@param {type:\"string\"}\n",
        "OUTPUT_DIR = 'proc_data/reddit' #@param {type:\"string\"}\n",
        "PRETRAINED_MODEL_DIR = 'xlnet_cased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
        "CHECKPOINT_DIR = 'exp/reddit' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "AWWlTOthuSbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate predicted label\n",
        "train_command = \"python xlnet/run_classifier.py \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --eval_all_ckpt=True \\\n",
        "  --task_name=imdb \\\n",
        "  --data_dir=\"+DATA_DIR+\" \\\n",
        "  --output_dir=\"+OUTPUT_DIR+\" \\\n",
        "  --model_dir=\"+CHECKPOINT_DIR+\" \\\n",
        "  --uncased=False \\\n",
        "  --spiece_model_file=\"+PRETRAINED_MODEL_DIR+\"/spiece.model \\\n",
        "  --model_config_path=\"+PRETRAINED_MODEL_DIR+\"/xlnet_config.json \\\n",
        "  --init_checkpoint=\"+PRETRAINED_MODEL_DIR+\"/xlnet_model.ckpt \\\n",
        "  --max_seq_length=128 \\\n",
        "  --train_batch_size=8 \\\n",
        "  --eval_batch_size=8 \\\n",
        "  --num_hosts=1 \\\n",
        "  --num_core_per_host=1 \\\n",
        "  --learning_rate=2e-5 \\\n",
        "  --train_steps=4000 \\\n",
        "  --warmup_steps=500 \\\n",
        "  --save_steps=500 \\\n",
        "  --iterations=500\"\n",
        "\n",
        "! {train_command}"
      ],
      "metadata": {
        "id": "J3Tnyv_ErWGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - Neutral, 1 - Postive, -1 - Negative\n",
        "sent_data = pd.read_csv(\"Reddit_Data.csv\")\n",
        "sent_data=sent_data.dropna()"
      ],
      "metadata": {
        "id": "M2lbZl2fJ_5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences  = []\n",
        "sent_data = sent_data[sent_data['category'] !=0]\n",
        "# for sentence in sent_data['clean_comment']:\n",
        "#   sentence = sentence + \"[SEP] [CLS]\"\n",
        "#   sentences.append(sentence)\n",
        "labels = sent_data['category'].values\n",
        "sentences = list(sent_data['clean_comment'])"
      ],
      "metadata": {
        "id": "FWF3SbvHJ__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128 "
      ],
      "metadata": {
        "id": "3e80SgcAhkmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Method 1\n",
        "\n",
        "def get_inputs(comments, tokenizer, max_len=MAX_LEN):\n",
        "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
        "    inps = [tokenizer.encode_plus(t, max_length=max_len,pad_to_max_length=True, add_special_tokens=True) for t in comments]\n",
        "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
        "    ids = np.array([a['attention_mask'] for a in inps])\n",
        "    segments = np.array([a['token_type_ids'] for a in inps])\n",
        "    return inp_tok, ids, segments\n",
        "\n",
        "def warmup(epoch, lr):\n",
        "    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n",
        "    However, as we are finetuning for few epoch it's not crucial.\n",
        "    \"\"\"\n",
        "    return max(lr +1e-6, 2e-5)\n",
        "\n",
        "def plot_metrics(pred, true_labels):\n",
        "    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n",
        "    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n",
        "    auc = roc_auc_score(true_labels, pred)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8,8))\n",
        "    ax.plot(fpr, tpr, color='red')\n",
        "    ax.plot([0,1], [0,1], color='black', linestyle='--')\n",
        "    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n",
        "    return fig"
      ],
      "metadata": {
        "id": "3YQyYMXPEuau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xlnet(mname):\n",
        "    \"\"\" Creates the model. It is composed of the XLNet main block and then\n",
        "    a classification head its added\n",
        "    \"\"\"\n",
        "    # Define token ids as inputs\n",
        "    word_inputs = tf.keras.Input(shape=(MAX_LEN,), name='word_inputs', dtype='int32')\n",
        "\n",
        "    # Call XLNet model\n",
        "    xlnet = TFXLNetModel.from_pretrained(mname)\n",
        "    xlnet_encodings = xlnet(word_inputs)[0]\n",
        "\n",
        "    # CLASSIFICATION HEAD \n",
        "    # Collect last step from last hidden state (CLS)\n",
        "    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
        "    # Apply dropout for regularization\n",
        "    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
        "    # Final output \n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n",
        "\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "FknZKRq70sUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlnet_model = 'xlnet-large-cased'\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)\n",
        "xlnet = create_xlnet(xlnet_model)\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.15, random_state=122)\n",
        "train_df = pd.DataFrame(X_train)\n",
        "train_df['target'] = y_train\n",
        "\n",
        "eval_df = pd.DataFrame(X_test)\n",
        "eval_df['target'] = y_test\n",
        "\n",
        "inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)"
      ],
      "metadata": {
        "id": "-8As7CrgDY5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n",
        "]"
      ],
      "metadata": {
        "id": "KhuKQMVhDmHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = xlnet.fit(x=inp_tok, y=y_train, epochs=1, batch_size=16, validation_split=.3, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "TU5VGzYTDmgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from pylab import rcParams\n",
        "from torch import nn, optim\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "slElo70DLEHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 73\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "df = shuffle(sent_data)"
      ],
      "metadata": {
        "id": "z50mrrgXYzHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "MAX_LEN = 512\n",
        "class ImdbDataset(Dataset):\n",
        "\n",
        "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])\n",
        "        target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=False,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        )\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype = 'int64')\n",
        "        input_ids = torch.tensor(input_ids) \n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "        attention_mask = torch.tensor(attention_mask)       \n",
        "\n",
        "        return {\n",
        "        'review_text': review,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask.flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "OYOTyZ0RZ8pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.5, random_state=101)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=101)"
      ],
      "metadata": {
        "id": "v5hKfNvdaTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = ImdbDataset(\n",
        "    reviews=df.clean_comment.to_numpy(),\n",
        "    targets=df.category.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )"
      ],
      "metadata": {
        "id": "5aOSQH3_aWha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "GVi1BG2maXDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetForSequenceClassification\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "E6Jg0yK0aaSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-6)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "id": "Lz5Jz5I8aqeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "  \n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].reshape(4,512).to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # preds = preds.cpu().detach().numpy()\n",
        "        _, prediction = torch.max(outputs[1], dim=1)\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "        prediction = prediction.cpu().detach().numpy()\n",
        "        accuracy = metrics.accuracy_score(targets, prediction)\n",
        "\n",
        "        acc += accuracy\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        counter = counter + 1\n",
        "\n",
        "    return acc / counter, np.mean(losses)"
      ],
      "metadata": {
        "id": "rO2vvJPUa0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "            _, prediction = torch.max(outputs[1], dim=1)\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "            prediction = prediction.cpu().detach().numpy()\n",
        "            accuracy = metrics.accuracy_score(targets, prediction)\n",
        "\n",
        "            acc += accuracy\n",
        "            losses.append(loss.item())\n",
        "            counter += 1\n",
        "\n",
        "    return acc / counter, np.mean(losses)"
      ],
      "metadata": {
        "id": "gTggPf9ra1f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,     \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler, \n",
        "        len(df_train),\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader, \n",
        "        device, \n",
        "        len(df_val)\n",
        "    )\n",
        "\n",
        "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/NLP/Sentiment Analysis Series/models/xlnet_model.bin')\n",
        "        best_accuracy = val_acc"
      ],
      "metadata": {
        "id": "U_YaThPba-gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(val_data_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hajfoK6icZN6",
        "outputId": "e1f83113-86fa-4902-9734-380f3a89465c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "targets = data['targets'].to(device)\n",
        "print(input_ids.reshape(4,512).shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "metadata": {
        "id": "u1ST6-sgeyej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6xvEaLipfJ4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
